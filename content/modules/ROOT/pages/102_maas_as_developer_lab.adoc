= Lab Exercise: Using MaaS as a Developer
:stem: latexmath
:icons: font
:toc: left
:source-highlighter: highlight.js
:numbered:

This exercise will guide you through the process of deploying AnythingLLM as a custom workbench in OpenShift AI and configuring it to connect to a Large Language Model (LLM) that is being served via a Model-as-a-Service (MaaS) pattern with 3scale managing API access.



image::102_maas_as_developer_01.png[] 
image::102_maas_as_developer_02.png[] 
image::102_maas_as_developer_03.png[] 
image::102_maas_as_developer_04.png[] 
image::102_maas_as_developer_05.png[] 
image::102_maas_as_developer_06.png[] 
image::102_maas_as_developer_07.png[] 
image::102_maas_as_developer_08.png[] 



Get model endpoint served by 3Scale:

[source,language,attributes]
----
oc get routes -n 3scale -o json | jq -r '.items[] | select(.spec.host | contains("llama-32-1b-instruct-cpu-maas-apicast-production")) | .spec.host'
----

Example:
https://llama-32-1b-instruct-cpu-maas-apicast-production.apps.cluster-hbp92.hbp92.sandbox1781.opentlc.com/v1




== *Step 1 Ensure Model is Served via MaaS (3scale) and Obtain API Key*

*Objective*: Before you can configure AnythingLLM, you need to ensure that the LLM you wish to use is accessible through a Model-as-a-Service (MaaS) setup, specifically one that leverages 3scale for API management. You will also obtain the necessary connection details.


1. Understand MaaS and 3scale's Role:
    * Model-as-a-Service (MaaS) is the architectural pattern we explained before.
    * 3scale acts as the primary entry point for users to quickly access AI models within the MaaS solution. It is a versatile component that authorizes and reports traffic to the model serving layer on OpenShift AI.
2. Obtain API Key and Endpoint Details:
    * Log in to the MaaS Platform (powered by 3scale): After logging into the MaaS platform, users can create a new application.
    * Select the Desired AI Model: Choose the specific AI model you are interested in using.
    * Retrieve Connection Details: The platform will instantly provide you with the necessary connection details. These include:
        ▪ Endpoint URL
        ▪ Model Name
        ▪ A unique API Key, which functions as an api_key parameter or an Authorization Bearer token for your API calls.
    * Record these details carefully, as you will need them for configuring AnythingLLM.

== Step 2: Add AnythingLLM as a Custom Workbench Image in OpenShift AI

*Objective*: AnythingLLM needs to be available as a selectable custom workbench image within your OpenShift AI environment.

1. Log in as Administrator: Log in to your OpenShift AI instance as rhoai-admin.
2. Navigate to Notebook Images Settings: From the left menu, navigate to Settings > Notebook Images.
3. Import New Image: Click on Import New Image.
4. Enter Image Location: In the provided field, enter quay.io/rh-aiservices-bu/anythingllm-workbench as the image location. This makes the AnythingLLM container image available cluster-wide for users to select in their projects.

== Step 3: Create and Launch the AnythingLLM Workbench

*Objective*: Now that the custom AnythingLLM image is available, you will create a new workbench instance using it.

1. Go to your Data Science Project: Navigate to your Data Science project within OpenShift AI.
2. Create a Workbench: Click on Create a Workbench.
3. Configure Workbench Details:
    * Name: Choose a descriptive name, such as "AnythingLLM".
    * Image selection: Select your custom-anythingLLM-image.
    * Deployment size: Choose Small (a GPU is not required for this specific setup).
    * Cluster storage: Adjust the size based on your data needs, particularly if you plan to use Retrieval-Augmented Generation (RAG).
    * Leave the remaining settings as default and click Create.
4. Launch the Workbench: Once the workbench shows that it is running, click Open.
5. Get Started: On the initial splash screen that appears, click on Get started.

== Step 4: Configure the LLM Endpoint within AnythingLLM

*Objective*: This is the critical step where you connect your AnythingLLM workbench to the LLM model served by 3scale, using the API key and endpoint details obtained in Phase 1.

1. Choose Provider: In the AnythingLLM configuration interface, select Generic OpenAI as the Provider from the available options.
2. Enter Base URL: Enter the Endpoint URL you obtained from 3scale as the Base URL. This URL should typically end with /v1 (e.g., https://mistral-7b-instruct-v0-3-mycluster.com:443/v1).
3. Enter API Key and Model Name: Enter the API Key and specify the Model Name that you received from 3scale.
4. Set Token Context Window and Max Tokens: Set the Token Context Window size and Max Tokens to be generated by default. A good starting point is 1024, and you can adjust this value later within your workspaces.


== Step 5: Final Setup and Interaction

*Objective*: Complete the AnythingLLM setup and begin using your private chatbot powered by the 3scale-served LLM.

1. Set Up User Access: On the next screen, select Just me. OpenShift's authentication already secures access to your workbench, so a secondary password is not typically necessary.
2. Review Configuration: Review the summary screen to confirm that all your settings are correct. You may skip any brief survey if prompted.
3. Create Workspace: Click on Create Workspace. This will set up a project area within AnythingLLM where you can organize different tasks and data.
4. Start Chatting: Navigate to your newly created workspace and begin interacting with the LLM. You can now explore the various features of AnythingLLM.