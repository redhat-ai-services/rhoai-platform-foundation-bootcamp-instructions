In this section we'll explore Model as a Service as an architectural pattern that allows organizations to access pre-selected, pre-trained, or custom-trained Large Language Models (LLMs) through an API, eliminating the need for individual teams to host these models themselves. 

MaaS acts as a centralized, shared resource for multiple teams, much like a coffee machine in an office breakroom. This approach positions an organization as its own internal AI provider, making LLMs accessible while optimizing GPU infrastructure (as you saw in GPU as a service).

First lets explore Challenges and Benefits surrounding this MaaS pattern.

== Infrastructure as a Service can be costly

Self-Service is good for plentiful resources & small teams…​ But:

* Throwing GPUs at the problem is risky
* Few people know how to use them correctly
* Leads to duplication and under-utilization
* Leads to high costs
* Most people want an LLM endpoint, not a GPU

image::100_infra_as_service.png[Infrastructure as a service] 

== Model as a Service (MaaS)

Offering AI models as a service to a larger audience, especially Large Language Models (LLMs), solves many of these problems:

* IT serves common models centrally
** Generative AI focus, applicable to any model
** Centralized pool of hardware
** Platform Engineering for AI
** AI management (versioning, regression testing, etc)

* Models available through API Gateway

* Developers consume models, build AI applications
** For end users (private assistants, etc)
** To improve products or services through AI

* Shared Resources business model keeps costs down

image::100_model_as_a_service.png[Model as a service] 


In the following sections you'll get to use a MaaS setup with Openshift AI which will give you a good overview of the solution achieving the benefits above. We'll follow these steps:

* Use the published model in a chat application

* Configure 3Scale API Gateway to expose the model

* Get some usage statistics from 3Scale

